{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Langchain, Pinecone and RAG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "698529924f21d7d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For this notebook, install the following \n",
    "!pip install python-dotenv\n",
    "!pip install pinecone-client \n",
    "!pip install langchain\n",
    "!pip install openai"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74e8cb54354e95d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install -U langchain-openai\n",
    "# !pip install neo4j"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e8421c8d4512265"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "# these are used throughput the notebook\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6800ff450c653765"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write data into pinecone "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df7826d38929aba6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Install libraries \n",
    "from pinecone import Pinecone\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1e0db98a25f754e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Here we load data into Pinecone\n",
    "\n",
    "# Step 1: Load environment variables\n",
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_KEY\")\n",
    "ai8_api_key = os.getenv(\"AI8_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "\n",
    "# Step 2: Check what exists in the database\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "time.sleep(1) # wait a moment for connection\n",
    "\n",
    "print(index.describe_index_stats())\n",
    "print(\"Successfully connected to the index\")\n",
    "\n",
    "\n",
    "# Step 3: Get the embeddings\n",
    "def get_embeddings(input_text):\n",
    "    url = \"https://llm.api.ai8.io/query_llm\"\n",
    "    data = {\n",
    "        'model':\"text-embedding-ada-002\",\n",
    "        'input':input_text,\n",
    "        'encoding_format':\"float\"\n",
    "    }\n",
    "    headers = {'Authorization': ai8_api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    response_data = json.loads(response.content)[\"data\"]\n",
    "    # Extract embeddings from the response\n",
    "    embeddings = [data['embedding'] for data in response_data]\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# Step 4: Read and prepare data\n",
    "file_path = \"supply_chain_textual_representations.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "# Step 5: Generate embeddings and import into Pinecone\n",
    "for idx, line in enumerate(lines):\n",
    "    print(f\"Processing line {idx + 1}/{len(lines)}\")\n",
    "    # Generate embedding using AI8\n",
    "    embedding = get_embeddings([line.strip()])  \n",
    "\n",
    "    if embedding is not None and embedding.size > 0:\n",
    "        # Normalize the embedding to unit length (optional)\n",
    "        embedding = embedding / np.linalg.norm(embedding, axis=1).reshape(-1, 1)\n",
    "\n",
    "        # Prepare data for insertion into Pinecone\n",
    "        vector_id = str(idx)  # Using line index as a unique identifier\n",
    "        # Include metadata; in this case, the original line text\n",
    "        data = [(vector_id, embedding.flatten().tolist(), {\"text\": line.strip()})]\n",
    "\n",
    "        # Insert the data into Pinecone\n",
    "        index.upsert(vectors=data)\n",
    "        # print(f\"Inserted line {idx + 1} into Pinecone index.\")\n",
    "    else:\n",
    "        print(f\"Failed to process line {idx + 1}.\")\n",
    "\n",
    "print(\"All lines processed and inserted into Pinecone.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c07f43b74079133"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 6: Query the Pinecone database\n",
    "# Query text\n",
    "query_text = \"Who does Natoora supply to and who supplies to them\"\n",
    "print(f\"The query asked: {query_text}\")\n",
    "\n",
    "# Generate the query embedding\n",
    "query_embedding = get_embeddings([query_text])\n",
    "\n",
    "if query_embedding is not None and query_embedding.size > 0:\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1).reshape(-1, 1)\n",
    "    query_vector = query_embedding.flatten().tolist()\n",
    "\n",
    "    res = index.query(vector=query_vector, top_k=10, include_metadata=True)\n",
    "    for match in res.get(\"matches\", []):\n",
    "        score = match.get(\"score\")\n",
    "        vector_id = match.get(\"id\")\n",
    "        metadata = match.get(\"metadata\", {})\n",
    "        text = metadata.get(\"text\", \"No metadata text available\")\n",
    "        print(f\"Score: {score:.2f}, ID: {vector_id}, Text: {text}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "940e11e9cb5ff6f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this stage, we've confirmed that the pinecone database is functioning correctly. Now, we'll proceed to utilize the retrievals from the pinecone database for our RAG implementation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7738bd58e18c2fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First RAG Attempt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4ef7df458f8fb3c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Define the Query\n",
    "rag_query_text = \"Who are Lina Store's suppliers?\"  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619422fe4162a113"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: Retrieve Embeddings for the Query\n",
    "query_embedding = get_embeddings([rag_query_text])\n",
    "\n",
    "if query_embedding is not None and query_embedding.size > 0:\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1).reshape(-1, 1)\n",
    "    query_vector = query_embedding.flatten().tolist()\n",
    "\n",
    "# Print statement for progress\n",
    "print(query_vector[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd9385829f741e2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: Retrieve Relevant Documents from Pinecone\n",
    "top_k = 10\n",
    "res = index.query(vector=query_vector, top_k=top_k, include_metadata=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0baf7931a3648ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 4: Prepare Augmented Query\n",
    "# This step gets a list of the retrived text \n",
    "# Extract texts from the matches\n",
    "contexts = [match[\"metadata\"][\"text\"] for match in res.get(\"matches\", [])]\n",
    "\n",
    "# Combine retrieved contexts with the original query\n",
    "augmented_query = \"\\n\\n---\\n\\n\".join(contexts) + \"\\n\\n---\\n\\n\" + rag_query_text\n",
    "\n",
    "print(augmented_query)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44443ce014ce6002"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 5: Generate Response \n",
    "\n",
    "# First let's create a system message to prime the model\n",
    "primer = f\"\"\"You are Q&A bot designed to answer questions about restaurant supply chains. \n",
    "You are highly intelligent and you answer user questions based on the information provided \n",
    "by the user above each question. If the information can not be found in the information\n",
    "provided by the user you truthfully say \"I don't know\". Please note that 'T2_Suppliers' \n",
    "refers to Tier 2 Suppliers, indicating they are a restaurant's suppliers' suppliers. \n",
    "This shows we are going further down the supply chain. \n",
    "Keep this in mind when formulating your response.\n",
    "\"\"\"\n",
    "\n",
    "def extract_message_oai(response_data):\n",
    "    message_content = response_data.get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    # format the extracted message as markdown\n",
    "    markdown_content = \"---\\n\\n\" + message_content + \"\\n\\n---\"\n",
    "    return markdown_content\n",
    "\n",
    "def generate_response_with_rag(augmented_text):\n",
    "    model = \"gpt-4\"\n",
    "    url = \"https://llm.api.ai8.io/query_llm\" \n",
    "    data = {\n",
    "        \"model\": model, \n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": primer},\n",
    "            {\"role\": \"user\", \"content\": augmented_text}\n",
    "        ]\n",
    "    }\n",
    "    headers = {'Authorization': ai8_api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_data = json.loads(response.content)\n",
    "        model_response = extract_message_oai(response_data)\n",
    "        return model_response\n",
    "    else:\n",
    "        return {\"statusCode\": response.status_code, \"body\": response.content}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a22ea6a9f3551392"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this primer, we've introduced Guardrails. These are instructions for the LLM model to follow when it doesn't have the necessary information to answer a question, which helps prevent the model from generating incorrect responses and avoids the occurrence of hallucinations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "659f039c0fda4c18"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 6: Display or Use the Generated Response\n",
    "generated_response = generate_response_with_rag(augmented_query)\n",
    "print(generated_response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e3eabd91d016e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was the final response from our initial RAG attempt. Now, let's compare this with outputs generated without using a RAG approach. We'll do this in two ways:\n",
    "\n",
    "1. We'll exclude the \"retrieval\" step, meaning the LLM won't receive any extra information in the prompt.\n",
    "2. We'll send the prompt to the LLM without providing a detailed primer, simulating what the answer would be like if we used a platform like 'ChatGPT', for example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f488c25c65c17bd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is a non-Augmented Query, we pass in the original query as the input\n",
    "non_augmented_response = generate_response_with_rag(rag_query_text)\n",
    "print(non_augmented_response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32edf1fda7814d7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the get_resp_oai but this time with a more simple primer. \n",
    "def get_resp_oai(input_text, model):\n",
    "    url = \"https://llm.api.ai8.io/query_llm\"\n",
    "    data = {\n",
    "        # Specify the model that you want to use\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a highly intelligent Q&A bot designed to answer questions about restaurant supply chains.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "    }\n",
    "    headers = {'Authorization': ai8_api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_data = json.loads(response.content)\n",
    "        model_response = extract_message_oai(response_data)\n",
    "        return model_response\n",
    "    else:\n",
    "        return {\"statusCode\": response.status_code, \"body\": response.content}\n",
    "\n",
    "non_augmented_response_2 = get_resp_oai(rag_query_text, \"gpt-4\")\n",
    "print(non_augmented_response_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "283cfd6e05682dd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG approach with LangChain Framework"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9928f5a5cd3688a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have successfully executed the first RAG approach, the next step is to incorporate an LLM framework - specifically LangChain. This offers two significant benefits:\n",
    "\n",
    "1. It provides a technical advantage by providing a standard interface for many use cases, and generally makes using LLMs more simple and efficient. \n",
    "2. It offers an opportunity to explore and learn various functionalities of LangChain.\n",
    "\n",
    "Research into LangChain has revealed six key features: Prompts, LLMs, Indexes, Memory, Chains and Agents. This section will aim to explore as many of these features as possible in an exploratory and iterative learning manner before consolidating the understanding at the end in a final RAG approach for this supply chain use case. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb7c02e437d9922d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f151f6af2645e09"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from langchain import PromptTemplate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c2eb1d71e480897"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We are going to first get familiar with works with PromptTemplates \n",
    "# Then we can draw on some more functionality that LangChain offers \n",
    "\n",
    "# Step 1: Environment variables \n",
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_KEY\")\n",
    "ai8_api_key = os.getenv(\"AI8_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abd353ca7e1b9419"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We need the get_embeddings function created at the beginning of the notebook \n",
    "# Run that to continue "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0187874a3cb9c8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: Augmented query function\n",
    "def generate_augmented_prompt(prompt, top_k=20):\n",
    "    # Step a: Retrieve embeddings for the question\n",
    "    query_embedding = get_embeddings([prompt])\n",
    "    \n",
    "    if query_embedding is not None and query_embedding.size > 0:\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1).reshape(-1, 1)\n",
    "        query_vector = query_embedding.flatten().tolist()\n",
    "    \n",
    "    # Step b: Retrieve relevant documents from Pinecone\n",
    "    res = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    # Extract texts from the matches\n",
    "    contexts = [match[\"metadata\"][\"text\"] for match in res.get(\"matches\", [])]\n",
    "    \n",
    "    # Step c: Prepare augmented query\n",
    "    # Combine retrieved contexts with the original question\n",
    "    augmented_prompt = \"\\n\\n---\\n\\n\".join(contexts) + \"\\n\\n---\\n\\n\" + prompt\n",
    "    \n",
    "    return augmented_prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "789b83813b3f1762"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define a simple prompt template \n",
    "template = \"\"\"Question: Tell me about the suppliers and tier 2 suppliers of {restaurant}.\"\"\"\n",
    "\n",
    "# Generate the prompt based on the template \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"restaurant\"])\n",
    "\n",
    "# Get the user to enter a restaurant name\n",
    "user_input_restaurant = input(\"Enter the name of the restaurant: \")\n",
    "\n",
    "# Use the user input in formatting the prompt\n",
    "formatted_prompt = prompt.format(restaurant=user_input_restaurant)\n",
    "\n",
    "# Say we want to keep a consistent restaurant, comment out line below \n",
    "# formatted_prompt = prompt.format(restaurant=\"The Chiltern Firehouse\")\n",
    "\n",
    "# This gets the final prompt that includes all the extra useful information from the pinecone database \n",
    "augmented_text = generate_augmented_prompt(formatted_prompt, top_k=10)\n",
    "\n",
    "# Finally we call the llm model with the prompt including the question and all relevant info \n",
    "response = generate_response_with_rag(augmented_text)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfc31602fb99e6d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the code blocks above utilized Prompt Templates from LangChain, below we'll streamline the process further by harnessing more of LangChain's capabilities. This includes directly interfacing with an LLM, ensuring consistency with langchain.schema, and constructing a RAG pipeline that chains together different components and operations, demonstrating the flexibility and power of combining various APIs to achieve our outcome. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77bf4d8df2fa99df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d36b889c1f5cd2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Rag pipeline \n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI Embeddings Model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index,\n",
    "    embed_model,\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "# Define the retriever with 'k=10' to retrieve top 10 similar documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Define the Large Language Model (LLM) with OpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define the prompt template for generating answers based on retrieved context\n",
    "template = \"\"\"You are a highly intelligent Q&A bot designed to answer questions about restaurant supply chains.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If the information cannot be found in the information provided, truthfully say \"I don't know\".\n",
    "Please note that 'T2_Suppliers' refers to Tier 2 Suppliers, indicating they are a restaurant's suppliers' suppliers.\n",
    "No pre-amble in the answer.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_pipeline = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "query = \"What restaurants are supplied by 'Hg Walter'?\"\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "answers.append(rag_pipeline.invoke(query))\n",
    "contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "916d6a9437fcd96f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LLMs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f97bff182540"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "580d2cb23bc8d455"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Here we just explore using llm's with a direct OpenAI API key\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Let's just test \n",
    "llm.invoke(\"How can I use docker to containerise my RAG API? Answer in bullet point steps\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1052d12c450cda5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chains"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92ba39c29b129f14"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain.chains import LLMChain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e632a7a10e8699"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before working with LangChain's LLMChain function, this section will initially delve into the underlying logic through a manual prompt engineering approach.\n",
    "\n",
    "In this approach, we'll first identify direct suppliers for a given entity and then proceed to explore their second-tier suppliers individually. \n",
    "\n",
    "This sequential exploration intuitively allows for a more focused and granular understanding of the supplier network. By breaking down the analysis into smaller steps, we can potentially achieve more accurate and comprehensive outputs. Each augmented prompt, created with data from the Pinecone database, is tailored to address smaller, more focused questions. This approach could avoid dealing with too much information at once and instead and capture specific details more effectively. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7eae08591e77729"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "# Template for generating a prompt to identify direct suppliers for a given entity\n",
    "supplier_template = \"\"\"Please determine and compile a list of all direct suppliers for {entity}, using the detailed supplier information provided. \n",
    "Focus exclusively on direct suppliers, which are specifically designated as T2_Suppliers. Direct suppliers are entities that directly provide goods or services to {entity}. \n",
    "If no T2_Supplier is found to directly supply to {entity}, the response should be left blank.\n",
    "\n",
    "For clarity, if you are asked to identify direct suppliers of Fresh Direct and you encounter information stating, 'A T2_Supplier named Sun Salads directly supplies Watercress in Dorset, UK, to a Supplier named Fresh Direct,' \n",
    "your task is to recognize 'Sun Salads' as the direct supplier to 'Fresh Direct'. Any other information is irrelevant and should not be included in your answer.\"\"\"\n",
    "\n",
    "def prompt_chain_function(template, entity, top_k):\n",
    "    # Generate prompt with the given template and entity\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"entity\"])\n",
    "    formatted_prompt = prompt.format(entity=entity)\n",
    "    # Augment the prompt text\n",
    "    augmented_text = generate_augmented_prompt(formatted_prompt, top_k=top_k)\n",
    "    # Generate response using RAG\n",
    "    response = generate_response_with_rag(augmented_text)\n",
    "    return response \n",
    "\n",
    "def clean_response_function(response):\n",
    "    # Template for cleaning response\n",
    "    template = \"\"\"Extract the names of suppliers from this text:\n",
    "    {text}\n",
    "    The output should strictly consist of a comma-separated list of the direct suppliers' names, without any additional text or explanations.\n",
    "    \n",
    "    For example, the output should look like this: 'Supplier_1, Supplier_2, ..., Supplier_n'.\n",
    "    Please ensure the list is formatted exactly as shown in the example, with each supplier's name separated by a comma and a space. \n",
    "    If there is no information, just write 'unknown'.\"\"\"\n",
    "    \n",
    "    # Generate prompt with the given response\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    formatted_prompt = prompt.format(text=response)\n",
    "    # Generate cleaned response using RAG\n",
    "    cleaned_response = generate_response_with_rag(formatted_prompt)\n",
    "    return cleaned_response\n",
    "\n",
    "def process_supplier_info(info):\n",
    "    # Removing unwanted parts from the string\n",
    "    cleaned_info = info.replace('---\\n\\n', '').replace('\\n\\n---', '').strip()\n",
    "    # Additional processing logic can be added here\n",
    "    return cleaned_info\n",
    "\n",
    "### Step 1: Get the suppliers of the restaurant and clean the output \n",
    "restaurant_template = \"\"\"Question: Tell me about the direct suppliers {entity}.\"\"\"  \n",
    "user_input_restaurant = input(\"Enter the name of the restaurant: \")\n",
    "restaurant_suppliers = prompt_chain_function(restaurant_template, user_input_restaurant, 10)\n",
    "clean_restaurant_suppliers = clean_response_function(restaurant_suppliers)\n",
    "clean_restaurant_suppliers = process_supplier_info(clean_restaurant_suppliers)\n",
    "\n",
    "# Print original and cleaned responses\n",
    "print(restaurant_suppliers)\n",
    "print(f\"{user_input_restaurant}:\\n{clean_restaurant_suppliers}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "### Step 2: Get the suppliers of the restaurant suppliers and clean the output \n",
    "suppliers_list = clean_restaurant_suppliers.split(', ')\n",
    "suppliers_list = [supplier.replace('---\\n\\n', '').replace('\\n\\n---', '') for supplier in suppliers_list]\n",
    "t2_supplier_outputs = {}\n",
    "for supplier in suppliers_list:\n",
    "    # Generate prompt to identify T2 suppliers for each restaurant supplier\n",
    "    output = prompt_chain_function(supplier_template, supplier, 20)\n",
    "    t2_supplier_outputs[supplier] = output\n",
    "\n",
    "# Process and clean T2 supplier outputs\n",
    "for key, value in t2_supplier_outputs.items():\n",
    "    processed_output = process_supplier_info(value)\n",
    "    clean_t2_suppliers = clean_response_function(processed_output)\n",
    "    final_output = process_supplier_info(clean_t2_suppliers)\n",
    "    \n",
    "    # Print T2 supplier outputs\n",
    "    print(f\"{key}'s suppliers: \\n{final_output}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c4c678d7aa4aa72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we can confirm this has worked, and from manual review, the answer is accurate and includes all relevant information, the next stage is to see if we can replicate the idea of where one answer feeds into another, but we will LangChain's LLMChain function. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36f6f5d4abc1c852"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We use the generate_augmented_prompt function defined above \n",
    "# Run that function before continuing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a6c83fd4927623f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# First, we are going to use LLMChains\n",
    "# This allows  for the creation of sequences (chains) where the output of one model can serve as the input for another, \n",
    "# enabling complex, multistep information processing workflows.\n",
    "# This is akin to prompt engineering \n",
    "\n",
    "# Step 1: Define the templates\n",
    "template_level1 = \"\"\"\"Identify and list all direct suppliers of {entity}, based on the provided supplier information. \n",
    "It's important to note that we are specifically looking for direct suppliers only, not Tier 2 Suppliers (also known as T2 Suppliers). \n",
    "The output should strictly consist of a comma-separated list of the direct suppliers' names, without any additional text or explanations.\n",
    "\n",
    "For example, the output should look like this: 'Supplier_1, Supplier_2, ..., Supplier_n'.\n",
    "Please ensure the list is formatted exactly as shown in the example, with each supplier's name separated by a comma and a space. \n",
    "\n",
    "Use the following supplier information to support your answer:\n",
    "{supplier_info}\n",
    "\"\"\"\n",
    "\n",
    "template_level2 = \"\"\"\"Please determine and compile a list of all direct suppliers for {entity}, using the detailed supplier information provided. \n",
    "Focus exclusively on direct suppliers, which are specifically designated as T2_Suppliers. Direct suppliers are entities that directly provide goods or services to {entity}. \n",
    "If no T2_Supplier is found to directly supply {entity}, the response should be left blank.\n",
    "\n",
    "For clarity, if you are asked to identify direct suppliers of Fresh Direct and you encounter information stating, 'A T2_Supplier named Sun Salads directly supplies Watercress in Dorset, UK, to a Supplier named Fresh Direct,' \n",
    "your task is to recognize 'Sun Salads' as the direct supplier to 'Fresh Direct'. Any other information is irrelevant and should not be included in your answer. \n",
    "\n",
    "Your response must be formatted as a comma-separated list containing the names of these direct suppliers, without including any additional text or explanations. \n",
    "\n",
    "Ensure your output adheres to the following format: 'Supplier_1, Supplier_2, ..., Supplier_n'. Each supplier's name should be clearly delineated, separated by a comma followed by a space. \n",
    "Use the following supplier information to support your answer:\n",
    "{supplier_info}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e9eb9aeafd78cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: Create the LLM to use \n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=openai_api_key)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f15c4e60d0d78a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: Generate the functions \n",
    "def generate_t1_suppliers(template, top_k, user_input_restaurant):\n",
    "    template = template \n",
    "    rag_template = \"\"\"List all of the suppliers of {entity}.\"\"\"\n",
    "    rag_prompt = PromptTemplate(input_variables=[\"entity\"], template=rag_template) \n",
    "\n",
    "    formatted_prompt = rag_prompt.format(entity=user_input_restaurant)\n",
    "    restaurant_suppliers = generate_augmented_prompt(formatted_prompt, top_k=top_k)\n",
    "\n",
    "    # Now we call the llm with the query \n",
    "    prompt = PromptTemplate(input_variables=[\"entity\", \"supplier_info\"], template=template) \n",
    "    suppliers_output = LLMChain(llm=llm, prompt=prompt, output_key='suppliers')\n",
    "    t1_output = suppliers_output.run(entity=user_input_restaurant, supplier_info=restaurant_suppliers)\n",
    "    \n",
    "    return t1_output\n",
    "\n",
    "\n",
    "# The function for tier 2 suppliers\n",
    "def generate_t2_suppliers(template, top_k, output=None):\n",
    "    template = template \n",
    "    rag_template = \"\"\"List all of the T2_Suppliers of {entity}.\"\"\"\n",
    "    rag_prompt = PromptTemplate(input_variables=[\"entity\"], template=rag_template) \n",
    "    # print(\"Going level 2 route\")\n",
    "    suppliers_list = output.split(', ')\n",
    "    t2_supplier_outputs = {}\n",
    "\n",
    "    for supplier in suppliers_list:\n",
    "        # print(supplier)\n",
    "        formatted_prompt = rag_prompt.format(entity=supplier)\n",
    "        supplier_suppliers = generate_augmented_prompt(formatted_prompt, top_k=top_k)\n",
    "\n",
    "        # Now we call the llm with the query \n",
    "        prompt = PromptTemplate(input_variables=[\"entity\", \"supplier_info\"], template=template) \n",
    "        suppliers_output = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "        t2_output = suppliers_output.run(entity=supplier, supplier_info=supplier_suppliers)\n",
    "        t2_supplier_outputs[supplier] = t2_output\n",
    "\n",
    "    return t2_supplier_outputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c996d5c44d0113c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Run the functions\n",
    "user_input_restaurant = input(\"Enter the name of the restaurant: \")\n",
    "print(\"\\n\")\n",
    "\n",
    "output = generate_t1_suppliers(template=template_level1, top_k=10, user_input_restaurant=user_input_restaurant)\n",
    "\n",
    "print(f\"{user_input_restaurant}'s Tier 1 Suppliers are: {output}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "t2_supplier_outputs = generate_t2_suppliers(template=template_level1, top_k=10, output=output)\n",
    "\n",
    "print(t2_supplier_outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93e63890e676affe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92fd95038d083c31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This next section is an example of creating a retrieval-based conversational agent that can handle detailed inquiries by leveraging both document retrieval mechanisms and conversational context. It showcases how chains and memory (in the form of conversational history) are utilized together to create more sophisticated and context-aware responses.\n",
    "\n",
    "So, 'chat_history' is used to store and reference the conversation history as part of the input for the conversational retrival and the code also utilised multiple different types of chains such as 'create_stuff_documents_chain', 'create_retrieval_chain', 'create_history_aware_retriever'.\n",
    "\n",
    "This represents another use case and opportunity to learn how LangChain can be used for this an LLM solution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d30d8a8d906e567"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# a) Document chain creation\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# b) Retrieval chain creation\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# c) Conversational Retrieval Chain \n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# d) Example Invocation and Conversational Context Management\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bc0d1bda6946b01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1. Document chain creation \n",
    "# This section initializes a chain for processing documents. It involves creating a template for querying and instantiating a document chain.\n",
    "\n",
    "template = \"\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d21eafaeb50655d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2. Retrieval Chain Setup\n",
    "# # Setup for a chain dedicated to retrieving documents based on a given query. This includes initializing a retriever and creating a retrieval chain that utilizes the previously defined document chain.\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI Embeddings Model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Set up the vectorstore \n",
    "vectorstore = PineconeVectorStore(\n",
    "    index,\n",
    "    embed_model,\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Testing it works\n",
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"Who supplies Lina Stores?\"\n",
    "})\n",
    "lina_stores_answer = response['answer'] \n",
    "print(lina_stores_answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd9d7bda3b8fd1e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3. Conversational Retrieval Chain Initialization\n",
    "# Creating a chain that is aware of the conversation history for more contextual retrieval. This involves setting up templates and chains that take into account the chat history.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e98b2b809ec14a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 4: Example Invocation and Conversational Context Management\n",
    "# This section demonstrates how to invoke the conversational retrieval chain with a specific query and manage the chat history to simulate a dynamic conversation.\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Who supplies Lina Stores?\"),\n",
    "    AIMessage(content=lina_stores_answer)\n",
    "]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me more about it!\"\n",
    "})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "# Manually test \n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': [],\n",
    "    \"input\": \"Who supplies Lina Stores?\"\n",
    "})\n",
    "\n",
    "message_answer = response['answer']\n",
    "\n",
    "print(f\"The response output, including 'chat_history', 'input', 'context' and 'message':\\n {response}\\n\")\n",
    "\n",
    "print(f\"Only the response 'message' output:\\n {message_answer}\")\n",
    "\n",
    "# After first invocation\n",
    "chat_history.append(HumanMessage(content=\"Tell me more about it!\"))\n",
    "chat_history.append(AIMessage(content=message_answer))  # Assuming you add the actual response\n",
    "\n",
    "# Now the chat_history includes the new exchange, and we can invoke the chain again with updated context\n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    \"input\": \"Can you find any other restaurants supplied by those suppliers\"\n",
    "})\n",
    "\n",
    "answer = response['answer']\n",
    "print(f\"Printing the next answer in the conversation:\\n {answer}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9b5857dabd0c03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 5: Conversational Loop (Interactive Querying)\n",
    "# Interactive loop allowing users to ask questions in a conversational context. This section demonstrates how the system can handle live user input and respond accordingly.\n",
    "\n",
    "# Given that the conversational_retrieval_chain is already defined and initialized\n",
    "\n",
    "# Define the function to ask a question, reference chat history and retrieval_chain (from Pinecone vector database), print the answer and update chat history\n",
    "\n",
    "def ask_question(input_question):\n",
    "    global chat_history  # Reference the global chat history\n",
    "\n",
    "    # Invoke the conversational retrieval chain with the current chat history and the input question\n",
    "    response = conversational_retrieval_chain.invoke({\n",
    "        'chat_history': chat_history,\n",
    "        \"input\": input_question\n",
    "    })\n",
    "\n",
    "    answer = response['answer']\n",
    "    print(f\"AI: {answer}\")\n",
    "\n",
    "    # Update chat history with the new exchange\n",
    "    chat_history.append(HumanMessage(content=input_question))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "\n",
    "# Interactive loop for asking questions\n",
    "while True:\n",
    "    user_input = input(\"Ask a question: \")  # Get input question from the user\n",
    "    if user_input.lower() == \"quit\":  # Define a way to exit the loop\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    ask_question(user_input)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da307eeb6d1995d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indexes \n",
    "* Document loaders \n",
    "* Vector databases\n",
    "* Text splitters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fc03641f2764d24"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# a) vector database \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# b) document loaders \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# c) text splitters \n",
    "from langchain.text_splitter  import RecursiveCharacterTextSplitter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6b1c5bb5b51cc67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (a) Vector Database"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e0808dc13f59f93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI Embeddings Model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Set up the vectorstore \n",
    "vectorstore = PineconeVectorStore(\n",
    "    index,\n",
    "    embed_model,\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"Who supplies Lina Stores??\"\n",
    "\n",
    "# Test functionality to retrieve data from Pinecone directly using LangChain\n",
    "vectorstore.similarity_search(query, k=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "461a3f5f0acb5b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (b) Document Loaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c447d1dc1ea109e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Additional information to write up a sustainability report \n",
    "# This is for a more comprehensive example use case \n",
    "\n",
    "loader = WebBaseLoader(\"https://www.sustainableagriculture.eco/post/navigating-the-complexity-of-sustainable-food-supply-chains\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8671c896f2c6df5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (c) Text Splitters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6085795f70957c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Here let's split the docs up into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# print statement for progress\n",
    "print(documents)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a2d1293bbac5009"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example use case "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a22535e00bbf0301"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "from tqdm.auto import tqdm  # For progress indication"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91e126c66a1026e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# First we have to add all the data from documents into the Pinecone database \n",
    "\n",
    "# Step 1: Generate embeddings for your documents\n",
    "# Split documents might have resulted in a list of strings (documents)\n",
    "# Let's generate embeddings for these documents\n",
    "embeddings_list = []\n",
    "for doc in tqdm(documents):\n",
    "    embedding = embed_model.embed_text(doc)  # Ensure this method matches your actual embedding method\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "# Step 2: Prepare data for Pinecone\n",
    "# Creating a unique identifier for each document \n",
    "data_to_upsert = []\n",
    "for idx, (doc, emb) in enumerate(zip(documents, embeddings_list)):\n",
    "    unique_id = f\"doc_{idx}\"  # You might want to create more sophisticated IDs based on your documents\n",
    "    data_to_upsert.append((unique_id, emb, {\"text\": doc}))\n",
    "\n",
    "# Step 3: Upsert data into Pinecone\n",
    "# This can be done in batches to avoid overloading memory with large datasets\n",
    "batch_size = 100 \n",
    "for i in tqdm(range(0, len(data_to_upsert), batch_size)):\n",
    "    batch = data_to_upsert[i:i+batch_size]\n",
    "    index.upsert(vectors=batch)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75fe73b2f25fd5d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### Here we want to use the output so the LLM can act as sustainability officer for a restaurant \n",
    "# The final text is a sustainability paragraph on behalf of the restaurant \n",
    "\n",
    "# First let's create a system message to prime the model\n",
    "primer = f\"\"\"\n",
    "You are a sustainability officer for a restaurant, tasked with understanding and optimizing the supply chain for both environmental sustainability and human rights. \n",
    "Your role involves assessing the sources of ingredients used in the restaurant, focusing on the transparency and sustainability of these sources.\n",
    "\n",
    "If comprehensive details of the supply chain are available, utilize this information to highlight sustainable practices or identify areas for improvement. \n",
    "In cases where information about suppliers or their practices is incomplete, note the need to seek out this information to enhance supply chain transparency.\n",
    "\n",
    "Your approach should include:\n",
    "- Identifying known suppliers and their contributions to the restaurant's supply chain.\n",
    "- Acknowledging gaps in the supply chain information, especially regarding secondary suppliers or the origins of specific ingredients.\n",
    "- Emphasizing the importance of continuous improvement and investigation to ensure the sustainability and ethical standards of the supply chain are met.\n",
    "- Write 1-2 Paragraphs that can be included in the restaurant's annual sustainability report.\n",
    "\"\"\"\n",
    "\n",
    "# The function generate_response_with_rag needs to be ran before continuing \n",
    "\n",
    "# Combining the information\n",
    "# This info is taken from the prompt engineering approach \n",
    "combined_info = restaurant_suppliers + \"\\n\\nTier 2 Supplier Information:\\n\"\n",
    "\n",
    "for supplier, details in t2_supplier_outputs.items():\n",
    "    combined_info += f\"\\n{supplier}:\\n{details}\"\n",
    "\n",
    "generate_response_with_rag(combined_info)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d142e66a5c100b4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agents \n",
    "[insert text here]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2191ef102df7ceb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import libraries "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f23d21546e47b2a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
