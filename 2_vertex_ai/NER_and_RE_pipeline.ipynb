{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cebabe4",
   "metadata": {},
   "source": [
    "# Vertex AI and ML Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cef71",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b82e4",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e4ffbd-4d77-4074-b84a-a9a10fe0b606",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-07T14:35:53.876154Z",
     "start_time": "2024-04-07T14:35:53.871604Z"
    }
   },
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b255bf32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-east1'\n",
    "DATANAME = 'api-news-data'\n",
    "NOTEBOOK = 'supply_chain_pipeline_notebook'\n",
    "\n",
    "# Resources\n",
    "DEPLOY_COMPUTE = 'n1-standard-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00693f36",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e6b18c-6deb-41dc-a632-8b305705eeec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U google-cloud-pipeline-components -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bf8620",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-07T14:36:37.079848Z",
     "start_time": "2024-04-07T14:36:37.076613Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a4860",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f88c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14fbbc",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65b2223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = \"restaurant-supplier-bucket\"\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c68630e-4fe3-473d-abb3-16d39fb782e8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-07T14:36:20.872236Z",
     "start_time": "2024-04-07T14:36:20.868631Z"
    }
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d35908-9b69-40be-97dc-fb4fb0279709",
   "metadata": {},
   "source": [
    "List the service accounts current roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f46306-bd55-425a-951a-fd2753807b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE\n",
      "roles/editor\n",
      "roles/run.admin\n",
      "roles/storage.objectAdmin\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f15187",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7383876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c3499",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline (KFP) Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6b54e7-544a-44cb-808f-007ebc2c30cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install kfp google-cloud-pipeline-components google-cloud-storage openai pydantic\n",
    "# !pip install langchain langchain-openai\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, pipeline, InputPath, OutputPath\n",
    "from google.cloud import storage\n",
    "from typing import NamedTuple, List\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3e6d2be-520b-4c21-8dd9-2764f32da694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import component and OutputPath\n",
    "from kfp.v2.dsl import component, OutputPath\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"requests\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"kfp==2.7.0\",\n",
    "        \"click<9,>=8.0.0\",\n",
    "        \"docstring-parser<1,>=0.7.3\",\n",
    "        \"kfp-pipeline-spec==0.3.0\",\n",
    "        \"kfp-server-api<2.1.0,>=2.0.0\",\n",
    "        \"kubernetes<27,>=8.0.0\",\n",
    "        \"PyYAML<7,>=5.3\",\n",
    "        \"requests-toolbelt<1,>=0.8.0\",\n",
    "        \"tabulate<1,>=0.8.6\",\n",
    "        \"urllib3<2.0.0\", \n",
    "    ]\n",
    ")\n",
    "def fetch_and_store_articles(\n",
    "    news_api_key: str,\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    restaurants_csv_path: str,\n",
    "    output_file_path: OutputPath(\"json\")\n",
    "):\n",
    "    # Import libraries \n",
    "    import csv\n",
    "    import json\n",
    "    import requests\n",
    "    from google.cloud import storage\n",
    "    from io import StringIO\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    def read_restaurant_names_gcs(bucket_name, restaurants_csv_path):\n",
    "        blob = bucket.blob(restaurants_csv_path)\n",
    "        data = blob.download_as_text()\n",
    "        f = StringIO(data)\n",
    "        reader = csv.reader(f)\n",
    "        # next(reader, None)  # Skip the header\n",
    "        return [row[0] for row in reader]\n",
    "\n",
    "\n",
    "    restaurant_names = read_restaurant_names_gcs(bucket_name, restaurants_csv_path)\n",
    "    all_articles_data = []\n",
    "\n",
    "    for name in restaurant_names:\n",
    "        url = \"http://eventregistry.org/api/v1/article/getArticles\"\n",
    "        payload = {\n",
    "            \"keyword\": [name, \"suppliers\", \"restaurant\"],\n",
    "            \"keywordOper\": \"and\",\n",
    "            \"articlesPage\": 1,\n",
    "            \"articlesCount\": 100,\n",
    "            \"articlesSortBy\": \"date\",\n",
    "            \"articlesSortByAsc\": False,\n",
    "            \"resultType\": \"articles\",\n",
    "            \"dataType\": [\"news\"],\n",
    "            \"apiKey\": news_api_key,\n",
    "            \"includeArticleTitle\": True,\n",
    "            \"includeArticleBody\": True\n",
    "        }\n",
    "        response = requests.post(url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            articles = response.json().get('articles', {}).get('results', [])\n",
    "            for article in articles:\n",
    "                article_data = {\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"text\": article[\"body\"],\n",
    "                    \"relationships\": []\n",
    "                }\n",
    "                all_articles_data.append(article_data)\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(all_articles_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef27eca3-798e-4ef1-b80e-4188de42bbe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This could improve with parallel processing \n",
    "\n",
    "# Import for kfp\n",
    "from kfp.v2.dsl import component, InputPath, OutputPath\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\", \"openai\", \"pydantic\", \"langchain\", \"langchain-openai\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def process_articles_component(\n",
    "    input_json_path: InputPath('json'),\n",
    "    output_json_path: OutputPath('json'),\n",
    "    openai_api_key: str,\n",
    "    bucket_name: str,\n",
    "    project_id: str,\n",
    "    output_folder: str\n",
    "):\n",
    "    # Import libraries \n",
    "    from typing import List\n",
    "    import json\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    from pydantic import BaseModel, Field\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.output_parsers import PydanticOutputParser\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    class Relationship(BaseModel):\n",
    "        supplier: str = Field(description=\"Supplier Name\")\n",
    "        buyer: str = Field(description=\"Buyer Name\")\n",
    "        product: str = Field(description=\"Products involved in the transaction\")\n",
    "        location: str = Field(description=\"Supplier Location\")\n",
    "\n",
    "    class RelationshipsData(BaseModel):\n",
    "        url: str = Field(description=\"The URL of the article\")\n",
    "        text: str = Field(description=\"The text of the article\")\n",
    "        relationships: List[Relationship] = Field(description=\"List of buyer-supplier relationships\")\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Initialize OpenAI and LangChain components securely\n",
    "    try:\n",
    "        model = ChatOpenAI(api_key=openai_api_key, model=\"gpt-4\", temperature=0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing ChatOpenAI: {e}\")\n",
    "        raise\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Please help identify all buyer-supplier relationships present in the provided text. \n",
    "    The primary objective of this project is to pinpoint entities within the restaurant industry for NER purposes, and to delineate supply chains. \n",
    "    Extract instances where one organization provides goods or services to another, requiring the naming of both parties involved.\n",
    "    Important Instructions \n",
    "    1. Do not add the url and article text to the JSON output, but rather leave as \"input URL here\" and \"input text here\" respectively. \n",
    "    2. Do not add a buyer-supplier relationship unless specific names of buyer and supplier organisations can be found. \n",
    "       This means examples like \"Customers\", \"Popular Restuarant\" do not count and should be excluded. \n",
    "    \n",
    "\n",
    "    << FORMATTING >>\n",
    "    {format_instructions}\n",
    "\n",
    "    << INPUT >>\n",
    "    {url_text}\n",
    "\n",
    "    << OUTPUT>>\n",
    "    Organize your findings into a JSON object following this structure:\n",
    "\n",
    "    JSON\n",
    "    {{\n",
    "        \"url\": \"input URL here\",\n",
    "        \"text\": \"input text here\",\n",
    "        \"relationships\": [\n",
    "            {{\n",
    "                \"supplier\": \"Supplier Name\",\n",
    "                \"buyer\": \"Buyer Name\",\n",
    "                \"product\": \"Products involved\",\n",
    "                \"location\": \"Supplier Location\"\n",
    "            }}\n",
    "            // Additional entries if multiple relationships are found\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=RelationshipsData)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"url_text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "        \n",
    "    try:\n",
    "        with open(input_json_path, 'r') as f:\n",
    "            articles = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading input JSON from GCS: {e}\")\n",
    "        raise\n",
    "\n",
    "    processed_texts = []\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            processed_text = chain.invoke({\"url_text\": article[\"text\"]})\n",
    "\n",
    "            relationships_dicts = [relationship.dict() for relationship in processed_text.relationships]\n",
    "\n",
    "            article_data = {\n",
    "                \"url\": article[\"url\"],\n",
    "                \"text\": article[\"text\"],\n",
    "                \"relationships\": relationships_dicts  \n",
    "            }\n",
    "            processed_texts.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error processing article {article['url']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "            \n",
    "    try:\n",
    "        processed_texts_json = json.dumps(processed_texts, indent=2) \n",
    "        with open(output_json_path, 'w') as f:\n",
    "            f.write(processed_texts_json)\n",
    "        logging.info(\"Successfully processed articles and saved output.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving processed texts: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7add1e2e-a9d1-438c-95c2-5a91ebe20172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp.v2.dsl import InputPath, OutputPath, component\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}',\n",
    "    pipeline_root=f'{URI}/{TIMESTAMP}/kfp/'\n",
    ")\n",
    "def supply_chain_pipeline(\n",
    "    news_api_key: str,\n",
    "    openai_api_key: str,\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    restaurants_csv_path: str,\n",
    "    output_folder: str,\n",
    "):\n",
    "    # Component invocation \n",
    "    fetch_articles_task = fetch_and_store_articles(\n",
    "        news_api_key=news_api_key,\n",
    "        project_id=project_id,\n",
    "        bucket_name=bucket_name,\n",
    "        restaurants_csv_path=restaurants_csv_path,\n",
    "    )\n",
    "\n",
    "    # Use the output from the previous component as input for the next one\n",
    "    process_articles_task = process_articles_component(\n",
    "        input_json_path=fetch_articles_task.outputs['output_file_path'],  \n",
    "        openai_api_key=openai_api_key,\n",
    "        bucket_name=bucket_name,\n",
    "        project_id=project_id,\n",
    "        output_folder=output_folder,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7cda6",
   "metadata": {},
   "source": [
    "---\n",
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd56921b-8ce1-4622-88f5-6508c175287c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://temp/supply_chain_pipeline_notebook/supply_chain_pipeline_notebook.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 13.2 KiB/ 13.2 KiB]                                                \n",
      "Operation completed over 1 objects/13.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import kfp.compiler as compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=supply_chain_pipeline,  \n",
    "    package_path=f\"{DIR}/{NOTEBOOK}.yaml\"  # Or '.json' \n",
    ")\n",
    "\n",
    "# Move compiled pipeline files to GCS Bucket\n",
    "!gsutil cp {DIR}/{NOTEBOOK}.yaml {URI}/{TIMESTAMP}/kfp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45241ff1",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Vertex AI Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f92ac49-7425-44b3-b501-933658eab28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Adjust these variables based on your specific environment and pipeline\n",
    "PROJECT_ID = PROJECT_ID\n",
    "REGION = 'us-east1'\n",
    "PIPELINE_NAME = f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}'\n",
    "TEMPLATE_PATH = f\"{URI}/{TIMESTAMP}/kfp/{NOTEBOOK}.yaml\"  \n",
    "\n",
    "# Initialize the Vertex AI client\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Create and run the pipeline job\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    parameter_values={\n",
    "        'news_api_key': 'c3960efd-57ae-42e5-8bb1-adf076f9744c',\n",
    "        'openai_api_key': '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',\n",
    "        'project_id': PROJECT_ID,\n",
    "        'bucket_name': BUCKET,\n",
    "        'restaurants_csv_path': 'restaurant_names/restaurants_names_entry.csv',\n",
    "        'output_folder': f'{BUCKET}/processed_output',\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7093041-eace-46f6-9d40-0db44e68c598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955?project=557605249790\n",
      "PipelineJob projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/557605249790/locations/us-east1/pipelineJobs/kfp-supply-chain-pipeline-notebook-api-news-data-20240406203101-20240406211955\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline job\n",
    "response = pipeline_job.run(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "26a0497e-59f7-4f28-9227-e59d16e0e6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:bucket_name</th>\n",
       "      <th>param.input:news_api_key</th>\n",
       "      <th>param.input:project_id</th>\n",
       "      <th>param.input:openai_api_key</th>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <th>param.input:output_folder</th>\n",
       "      <th>param.input:restaurants_csv_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kfp-supply-chain-pipeline-notebook-api-news-da...</td>\n",
       "      <td>kfp-supply-chain-pipeline-notebook-api-news-da...</td>\n",
       "      <td>restaurant-supplier-bucket</td>\n",
       "      <td>c3960efd-57ae-42e5-8bb1-adf076f9744c</td>\n",
       "      <td>restaurant-supplier-networks</td>\n",
       "      <td>sk-flQQ9k4j6Go70wd6DfiUT3BlbkFJYZk9gUeV27KFPNF...</td>\n",
       "      <td>{'pipeline_run_component': {'location_id': 'us...</td>\n",
       "      <td>restaurant-supplier-bucket/processed_output</td>\n",
       "      <td>restaurant_names/restaurants_names_entry.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       pipeline_name  \\\n",
       "0  kfp-supply-chain-pipeline-notebook-api-news-da...   \n",
       "\n",
       "                                            run_name  \\\n",
       "0  kfp-supply-chain-pipeline-notebook-api-news-da...   \n",
       "\n",
       "      param.input:bucket_name              param.input:news_api_key  \\\n",
       "0  restaurant-supplier-bucket  c3960efd-57ae-42e5-8bb1-adf076f9744c   \n",
       "\n",
       "         param.input:project_id  \\\n",
       "0  restaurant-supplier-networks   \n",
       "\n",
       "                          param.input:openai_api_key  \\\n",
       "0  sk-flQQ9k4j6Go70wd6DfiUT3BlbkFJYZk9gUeV27KFPNF...   \n",
       "\n",
       "                     param.vmlmd_lineage_integration  \\\n",
       "0  {'pipeline_run_component': {'location_id': 'us...   \n",
       "\n",
       "                     param.input:output_folder  \\\n",
       "0  restaurant-supplier-bucket/processed_output   \n",
       "\n",
       "               param.input:restaurants_csv_path  \n",
       "0  restaurant_names/restaurants_names_entry.csv  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_df = aiplatform.get_pipeline_df(pipeline = \"kfp-supply-chain-pipeline-notebook-api-news-data-20240406164425\")\n",
    "pipeline_df.head(1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
