{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698529924f21d7d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Langchain, Pinecone and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e8cb54354e95d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # For this notebook, install the following \n",
    "# !pip install python-dotenv\n",
    "# !pip install pinecone-client \n",
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install -U langchain-openai\n",
    "# !pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8421c8d4512265",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6800ff450c653765",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "# these are used throughput the notebook\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7826d38929aba6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Write data into pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e0db98a25f754e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Install libraries \n",
    "from pinecone import Pinecone\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c07f43b74079133",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 868}},\n",
      " 'total_vector_count': 868}\n",
      "Successfully connected to the index\n",
      "All lines processed and inserted into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Here we load data into Pinecone\n",
    "\n",
    "# Step 1: Load environment variables\n",
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_KEY\")\n",
    "ai8_api_key = os.getenv(\"AI8_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "\n",
    "# Step 2: Check what exists in the database\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "time.sleep(1) # wait a moment for connection\n",
    "\n",
    "print(index.describe_index_stats())\n",
    "print(\"Successfully connected to the index\")\n",
    "\n",
    "\n",
    "# Step 3: Get the embeddings\n",
    "def get_embeddings(input_text):\n",
    "    url = \"https://llm.api.ai8.io/query_llm\"\n",
    "    data = {\n",
    "        'model':\"text-embedding-ada-002\",\n",
    "        'input':input_text,\n",
    "        'encoding_format':\"float\"\n",
    "    }\n",
    "    headers = {'Authorization': ai8_api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    response_data = json.loads(response.content)[\"data\"]\n",
    "    # Extract embeddings from the response\n",
    "    embeddings = [data['embedding'] for data in response_data]\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# Step 4: Read and prepare data\n",
    "file_path = \"supply_chain_textual_representations.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "# Step 5: Generate embeddings and import into Pinecone\n",
    "for idx, line in enumerate(lines):\n",
    "    print(f\"Processing line {idx + 1}/{len(lines)}\")\n",
    "    # Generate embedding using AI8\n",
    "    embedding = get_embeddings([line.strip()])  \n",
    "\n",
    "    if embedding is not None and embedding.size > 0:\n",
    "        # Normalize the embedding to unit length (optional)\n",
    "        embedding = embedding / np.linalg.norm(embedding, axis=1).reshape(-1, 1)\n",
    "\n",
    "        # Prepare data for insertion into Pinecone\n",
    "        vector_id = str(idx)  # Using line index as a unique identifier\n",
    "        # Include metadata; in this case, the original line text\n",
    "        data = [(vector_id, embedding.flatten().tolist(), {\"text\": line.strip()})]\n",
    "\n",
    "        # Insert the data into Pinecone\n",
    "        index.upsert(vectors=data)\n",
    "        # print(f\"Inserted line {idx + 1} into Pinecone index.\")\n",
    "    else:\n",
    "        print(f\"Failed to process line {idx + 1}.\")\n",
    "\n",
    "print(\"All lines processed and inserted into Pinecone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940e11e9cb5ff6f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query asked: Who does Natoora supply to and who supplies to them\n",
      "Score: 0.87, ID: 233, Text: A Supplier named Natoora supplies Produce in the location Not Specified to a Restaurant named Warehouse.\n",
      "Score: 0.86, ID: 236, Text: A Supplier named Natoora supplies Seasonal Produce in the location Uk to a Restaurant named Koya.\n",
      "Score: 0.85, ID: 235, Text: A Supplier named Natoora supplies Seasonal Produce in the location Uk to a Restaurant named Andrew Edmunds.\n",
      "Score: 0.85, ID: 169, Text: A T2_Supplier named Neals Yard Dairy supplies Dairy Products in the location Not Specified to a Supplier named Natoora.\n",
      "Score: 0.85, ID: 171, Text: A T2_Supplier named Alma supplies Bread in the location Bermondsey, London to a Supplier named Natoora.\n",
      "Score: 0.85, ID: 234, Text: A Supplier named Natoora supplies Seasonal Produce in the location Uk to a Restaurant named Cafe Murano.\n",
      "Score: 0.85, ID: 165, Text: A T2_Supplier named Timbarra Farm supplies Diverse Vegetables in the location Yarra Ranges, Melbourne to a Supplier named Natoora.\n",
      "Score: 0.85, ID: 439, Text: A T2_Supplier named Neals Yard Dairy supplies Dairy Products in the location Not Specified to a Supplier named Natoora, this Supplier named Natoora then supplies Seasonal Produce in the location Uk to a Restaurant named Koya.\n",
      "Score: 0.85, ID: 436, Text: A T2_Supplier named Neals Yard Dairy supplies Dairy Products in the location Not Specified to a Supplier named Natoora, this Supplier named Natoora then supplies Produce in the location Not Specified to a Restaurant named Warehouse.\n",
      "Score: 0.85, ID: 424, Text: A T2_Supplier named François Farm supplies Root Vegetables in the location Dunkirk, France to a Supplier named Natoora, this Supplier named Natoora then supplies Seasonal Produce in the location Uk to a Restaurant named Koya.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Query the Pinecone database\n",
    "# Query text\n",
    "query_text = \"Who does Natoora supply to and who supplies to them\"\n",
    "print(f\"The query asked: {query_text}\")\n",
    "\n",
    "# Generate the query embedding\n",
    "query_embedding = get_embeddings([query_text])\n",
    "\n",
    "if query_embedding is not None and query_embedding.size > 0:\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1).reshape(-1, 1)\n",
    "    query_vector = query_embedding.flatten().tolist()\n",
    "\n",
    "    res = index.query(vector=query_vector, top_k=10, include_metadata=True)\n",
    "    for match in res.get(\"matches\", []):\n",
    "        score = match.get(\"score\")\n",
    "        vector_id = match.get(\"id\")\n",
    "        metadata = match.get(\"metadata\", {})\n",
    "        text = metadata.get(\"text\", \"No metadata text available\")\n",
    "        print(f\"Score: {score:.2f}, ID: {vector_id}, Text: {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7738bd58e18c2fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "At this stage, we've confirmed that the pinecone database is functioning correctly. Now, we'll proceed to utilize the retrievals from the pinecone database for our RAG implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef7df458f8fb3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## First RAG Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619422fe4162a113",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define the Query\n",
    "rag_query_text = \"Who are Lina Store's suppliers?\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9385829f741e2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01693679864486545\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Retrieve Embeddings for the Query\n",
    "query_embedding = get_embeddings([rag_query_text])\n",
    "\n",
    "if query_embedding is not None and query_embedding.size > 0:\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1).reshape(-1, 1)\n",
    "    query_vector = query_embedding.flatten().tolist()\n",
    "\n",
    "# Print statement for progress\n",
    "print(query_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0baf7931a3648ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Retrieve Relevant Documents from Pinecone\n",
    "top_k = 10\n",
    "res = index.query(vector=query_vector, top_k=top_k, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44443ce014ce6002",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Supplier named Terre Di San Vito supplies Extra Virgin Olive Oil in the location Puglia to a Restaurant named Lina Stores.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named La Sovrana supplies Amalfi Lemons, Datterini Tomatoes, Buffalo Burrata, Aubergines in the location New Covent Garden Market, London to a Restaurant named Lina Stores.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Rocca Delle Macìe supplies Sangiovese Igt (red Wine), Vermentino Igt (white Wine) in the location Castellina in Chianti, Tuscany, Italy to a Restaurant named Lina Stores.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Natoora supplies Produce in the location Not Specified to a Restaurant named Warehouse.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Heilala Vanilla supplies Vanilla in the location Tonga, Pacific to a Restaurant named Warehouse.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Natoora supplies Seasonal Produce in the location Uk to a Restaurant named Cafe Murano.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Bocca Di Lupo.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Amaya.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Cinnamon Group.\n",
      "\n",
      "---\n",
      "\n",
      "A Supplier named Amary Farm supplies Vegetables in the location Lincombe, North Devon to a Restaurant named The Exmoor Forest Inn.\n",
      "\n",
      "---\n",
      "\n",
      "Who are Lina Store's suppliers?\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare Augmented Query\n",
    "# This step gets a list of the retrived text \n",
    "# Extract texts from the matches\n",
    "contexts = [match[\"metadata\"][\"text\"] for match in res.get(\"matches\", [])]\n",
    "\n",
    "# Combine retrieved contexts with the original query\n",
    "augmented_query = \"\\n\\n---\\n\\n\".join(contexts) + \"\\n\\n---\\n\\n\" + rag_query_text\n",
    "\n",
    "print(augmented_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a22ea6a9f3551392",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Generate Response \n",
    "\n",
    "# First let's create a system message to prime the model\n",
    "primer = f\"\"\"You are Q&A bot designed to answer questions about restaurant supply chains. \n",
    "You are highly intelligent and you answer user questions based on the information provided \n",
    "by the user above each question. If the information can not be found in the information\n",
    "provided by the user you truthfully say \"I don't know\". Please note that 'T2_Suppliers' \n",
    "refers to Tier 2 Suppliers, indicating they are a restaurant's suppliers' suppliers. \n",
    "This shows we are going further down the supply chain. \n",
    "Keep this in mind when formulating your response.\n",
    "\"\"\"\n",
    "\n",
    "def extract_message_oai(response_data):\n",
    "    message_content = response_data.get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    # format the extracted message as markdown\n",
    "    markdown_content = \"---\\n\\n\" + message_content + \"\\n\\n---\"\n",
    "    return markdown_content\n",
    "\n",
    "def generate_response_with_rag(augmented_text):\n",
    "    model = \"gpt-4\"\n",
    "    url = \"https://llm.api.ai8.io/query_llm\" \n",
    "    data = {\n",
    "        \"model\": model, \n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": primer},\n",
    "            {\"role\": \"user\", \"content\": augmented_text}\n",
    "        ]\n",
    "    }\n",
    "    headers = {'Authorization': ai8_api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_data = json.loads(response.content)\n",
    "        model_response = extract_message_oai(response_data)\n",
    "        return model_response\n",
    "    else:\n",
    "        return {\"statusCode\": response.status_code, \"body\": response.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f039c0fda4c18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this primer, we've introduced Guardrails. These are instructions for the LLM model to follow when it doesn't have the necessary information to answer a question, which helps prevent the model from generating incorrect responses and avoids the occurrence of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e3eabd91d016e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "Lina Store's suppliers are Terre Di San Vito, La Sovrana, and Rocca Delle Macìe.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Display or Use the Generated Response\n",
    "generated_response = generate_response_with_rag(augmented_query)\n",
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f488c25c65c17bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This was the final response from our initial RAG attempt. Now, let's compare this with outputs generated without using a RAG approach. We'll do this in two ways:\n",
    "\n",
    "1. We'll exclude the \"retrieval\" step, meaning the LLM won't receive any extra information in the prompt.\n",
    "2. We'll send the prompt to the LLM without providing a detailed primer, simulating what the answer would be like if we used a platform like 'ChatGPT', for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32edf1fda7814d7e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "I'm sorry, but the information provided does not include the details regarding Lina Store's suppliers.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# This is a non-Augmented Query, we pass in the original query as the input\n",
    "non_augmented_response = generate_response_with_rag(rag_query_text)\n",
    "print(non_augmented_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283cfd6e05682dd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "As an AI developed by OpenAI, I don't have real-time access to proprietary databases or business information of specific companies, so I can't provide the exact information about Lina Stores' suppliers. However, reputable restaurants and food businesses, like Lina Stores, typically source their ingredients from a mix of local and international suppliers, maintaining a specific focus on the quality and authenticity of ingredients. For detailed information, I recommend contacting Lina Stores directly or visiting their official website if this information is publicly available.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Define the get_resp_oai but this time with a more simple primer. \n",
    "def get_resp_oai(input_text, model):\n",
    "    url = \"https://llm.api.ai8.io/query_llm\"\n",
    "    data = {\n",
    "        # Specify the model that you want to use\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a highly intelligent Q&A bot designed to answer questions about restaurant supply chains.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "    }\n",
    "    headers = {'Authorization': ai8_api_key}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_data = json.loads(response.content)\n",
    "        model_response = extract_message_oai(response_data)\n",
    "        return model_response\n",
    "    else:\n",
    "        return {\"statusCode\": response.status_code, \"body\": response.content}\n",
    "\n",
    "non_augmented_response_2 = get_resp_oai(rag_query_text, \"gpt-4\")\n",
    "print(non_augmented_response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928f5a5cd3688a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## RAG approach with LangChain Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c02e437d9922d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now that we have successfully executed the first RAG approach, the next step is to incorporate an LLM framework - specifically LangChain. This offers two significant benefits:\n",
    "\n",
    "1. It provides a technical advantage by providing a standard interface for many use cases, and generally makes using LLMs more simple and efficient. \n",
    "2. It offers an opportunity to explore and learn various functionalities of LangChain.\n",
    "\n",
    "Research into LangChain has revealed six key features: Prompts, LLMs, Indexes, Memory, Chains and Agents. This section will aim to explore as many of these features as possible in an exploratory and iterative learning manner before consolidating the understanding at the end in a final RAG approach for this supply chain use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f151f6af2645e09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2eb1d71e480897",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd353ca7e1b9419",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We are going to first get familiar with works with PromptTemplates \n",
    "# Then we can draw on some more functionality that LangChain offers \n",
    "\n",
    "# Step 1: Environment variables \n",
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_KEY\")\n",
    "ai8_api_key = os.getenv(\"AI8_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0187874a3cb9c8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We need the get_embeddings function created at the beginning of the notebook \n",
    "# Run that to continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789b83813b3f1762",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Augmented query function\n",
    "def generate_augmented_prompt(prompt, top_k=20):\n",
    "    # Step a: Retrieve embeddings for the question\n",
    "    query_embedding = get_embeddings([prompt])\n",
    "    \n",
    "    if query_embedding is not None and query_embedding.size > 0:\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1).reshape(-1, 1)\n",
    "        query_vector = query_embedding.flatten().tolist()\n",
    "    \n",
    "    # Step b: Retrieve relevant documents from Pinecone\n",
    "    res = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    # Extract texts from the matches\n",
    "    contexts = [match[\"metadata\"][\"text\"] for match in res.get(\"matches\", [])]\n",
    "    \n",
    "    # Step c: Prepare augmented query\n",
    "    # Combine retrieved contexts with the original question\n",
    "    augmented_prompt = \"\\n\\n---\\n\\n\".join(contexts) + \"\\n\\n---\\n\\n\" + prompt\n",
    "    \n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfc31602fb99e6d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the restaurant:  The Chiltern Firehouse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "The Chiltern Firehouse has several suppliers that provide it with tea. One of them is the Rare Tea Company, which sources its tea from different T2_Suppliers. Jun Chiyabari, a T2_Supplier located in Dhankuta, Eastern Nepal; Satemwa Tea & Coffee Estate, a T2_Supplier located in Shire Highlands, Thyolo Mountains, Malawi; and Amba, a T2_Supplier located in Uva Highlands, Sri Lanka that not only provides handcrafted black teas and lemongrass but also fruits & vegetables, spices, honey, and homemade jams. These T2_Suppliers send tea to the Rare Tea Company, which then sells it to The Chiltern Firehouse. The delivery location for the tea from Rare Tea Company to The Chiltern Firehouse is not specified.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt template \n",
    "template = \"\"\"Question: Tell me about the suppliers and tier 2 suppliers of {restaurant}.\"\"\"\n",
    "\n",
    "# Generate the prompt based on the template \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"restaurant\"])\n",
    "\n",
    "# Get the user to enter a restaurant name\n",
    "user_input_restaurant = input(\"Enter the name of the restaurant: \")\n",
    "\n",
    "# Use the user input in formatting the prompt\n",
    "formatted_prompt = prompt.format(restaurant=user_input_restaurant)\n",
    "\n",
    "# Say we want to keep a consistent restaurant, comment out line below \n",
    "# formatted_prompt = prompt.format(restaurant=\"The Chiltern Firehouse\")\n",
    "\n",
    "# This gets the final prompt that includes all the extra useful information from the pinecone database \n",
    "augmented_text = generate_augmented_prompt(formatted_prompt, top_k=10)\n",
    "\n",
    "# Finally we call the llm model with the prompt including the question and all relevant info \n",
    "response = generate_response_with_rag(augmented_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf4d8df2fa99df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "While the code blocks above utilized Prompt Templates from LangChain, below we'll streamline the process further by harnessing more of LangChain's capabilities. This includes directly interfacing with an LLM, ensuring consistency with langchain.schema, and constructing a RAG pipeline that chains together different components and operations, demonstrating the flexibility and power of combining various APIs to achieve our outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36b889c1f5cd2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916d6a9437fcd96f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Rag pipeline \n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI Embeddings Model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index,\n",
    "    embed_model,\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "# Define the retriever with 'k=10' to retrieve top 10 similar documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Define the Large Language Model (LLM) with OpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define the prompt template for generating answers based on retrieved context\n",
    "template = \"\"\"You are a highly intelligent Q&A bot designed to answer questions about restaurant supply chains.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If the information cannot be found in the information provided, truthfully say \"I don't know\".\n",
    "Please note that 'T2_Suppliers' refers to Tier 2 Suppliers, indicating they are a restaurant's suppliers' suppliers.\n",
    "No pre-amble in the answer.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_pipeline = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "query = \"What restaurants are supplied by 'Hg Walter'?\"\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "answers.append(rag_pipeline.invoke(query))\n",
    "contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f97bff182540",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580d2cb23bc8d455",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1052d12c450cda5f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Create a Dockerfile in the root directory of your RAG API project\\n- Define a base image for your Docker container (e.g. using the official R image)\\n- Copy your RAG API files into the container\\n- Expose the port on which your API will run (e.g. port 8000)\\n- Install any necessary dependencies for your API using the package manager (e.g. install.packages() in R)\\n- Define the command to start your API (e.g. Rscript app.R)\\n- Build the Docker image using the docker build command\\n- Run the Docker container using the docker run command, mapping the exposed port to a port on your host machine\\n- Test your containerized RAG API to ensure it is running correctly\\n\\nNote: Make sure to follow best practices for Docker containerization, such as keeping your container lightweight and secure.', response_metadata={'token_usage': {'completion_tokens': 177, 'prompt_tokens': 25, 'total_tokens': 202}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-67061eb4-836f-463e-ae98-fb066289e53f-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we just explore using llm's with a direct OpenAI API key\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Let's just test \n",
    "llm.invoke(\"How can I use docker to containerise my RAG API? Answer in bullet point steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba39c29b129f14",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e632a7a10e8699",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eae08591e77729",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Before working with LangChain's LLMChain function, this section will initially delve into the underlying logic through a manual prompt engineering approach.\n",
    "\n",
    "In this approach, we'll first identify direct suppliers for a given entity and then proceed to explore their second-tier suppliers individually. \n",
    "\n",
    "This sequential exploration intuitively allows for a more focused and granular understanding of the supplier network. By breaking down the analysis into smaller steps, we can potentially achieve more accurate and comprehensive outputs. Each augmented prompt, created with data from the Pinecone database, is tailored to address smaller, more focused questions. This approach could avoid dealing with too much information at once and instead and capture specific details more effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4c678d7aa4aa72",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the restaurant:  The Chiltern Firehouse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "The Chiltern Firehouse has been mentioned in relation to a direct supplier called Rare Tea Company. This supplier provides them with Tea. The exact location where this transaction occurs is not specified.\n",
      "\n",
      "---\n",
      "The Chiltern Firehouse:\n",
      "Rare Tea Company\n",
      "\n",
      "\n",
      "Rare Tea Company's suppliers: \n",
      "Jun Chiyabari\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "# Template for generating a prompt to identify direct suppliers for a given entity\n",
    "supplier_template = \"\"\"Please determine and compile a list of all direct suppliers for {entity}, using the detailed supplier information provided. \n",
    "Focus exclusively on direct suppliers, which are specifically designated as T2_Suppliers. Direct suppliers are entities that directly provide goods or services to {entity}. \n",
    "If no T2_Supplier is found to directly supply to {entity}, the response should be left blank.\n",
    "\n",
    "For clarity, if you are asked to identify direct suppliers of Fresh Direct and you encounter information stating, 'A T2_Supplier named Sun Salads directly supplies Watercress in Dorset, UK, to a Supplier named Fresh Direct,' \n",
    "your task is to recognize 'Sun Salads' as the direct supplier to 'Fresh Direct'. Any other information is irrelevant and should not be included in your answer.\"\"\"\n",
    "\n",
    "def prompt_chain_function(template, entity, top_k):\n",
    "    # Generate prompt with the given template and entity\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"entity\"])\n",
    "    formatted_prompt = prompt.format(entity=entity)\n",
    "    # Augment the prompt text\n",
    "    augmented_text = generate_augmented_prompt(formatted_prompt, top_k=top_k)\n",
    "    # Generate response using RAG\n",
    "    response = generate_response_with_rag(augmented_text)\n",
    "    return response \n",
    "\n",
    "def clean_response_function(response):\n",
    "    # Template for cleaning response\n",
    "    template = \"\"\"Extract the names of suppliers from this text:\n",
    "    {text}\n",
    "    The output should strictly consist of a comma-separated list of the direct suppliers' names, without any additional text or explanations.\n",
    "    \n",
    "    For example, the output should look like this: 'Supplier_1, Supplier_2, ..., Supplier_n'.\n",
    "    Please ensure the list is formatted exactly as shown in the example, with each supplier's name separated by a comma and a space. \n",
    "    If there is no information, just write 'unknown'.\"\"\"\n",
    "    \n",
    "    # Generate prompt with the given response\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    formatted_prompt = prompt.format(text=response)\n",
    "    # Generate cleaned response using RAG\n",
    "    cleaned_response = generate_response_with_rag(formatted_prompt)\n",
    "    return cleaned_response\n",
    "\n",
    "def process_supplier_info(info):\n",
    "    # Removing unwanted parts from the string\n",
    "    cleaned_info = info.replace('---\\n\\n', '').replace('\\n\\n---', '').strip()\n",
    "    # Additional processing logic can be added here\n",
    "    return cleaned_info\n",
    "\n",
    "### Step 1: Get the suppliers of the restaurant and clean the output \n",
    "restaurant_template = \"\"\"Question: Tell me about the direct suppliers {entity}.\"\"\"  \n",
    "user_input_restaurant = input(\"Enter the name of the restaurant: \")\n",
    "restaurant_suppliers = prompt_chain_function(restaurant_template, user_input_restaurant, 10)\n",
    "clean_restaurant_suppliers = clean_response_function(restaurant_suppliers)\n",
    "clean_restaurant_suppliers = process_supplier_info(clean_restaurant_suppliers)\n",
    "\n",
    "# Print original and cleaned responses\n",
    "print(restaurant_suppliers)\n",
    "print(f\"{user_input_restaurant}:\\n{clean_restaurant_suppliers}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "### Step 2: Get the suppliers of the restaurant suppliers and clean the output \n",
    "suppliers_list = clean_restaurant_suppliers.split(', ')\n",
    "suppliers_list = [supplier.replace('---\\n\\n', '').replace('\\n\\n---', '') for supplier in suppliers_list]\n",
    "t2_supplier_outputs = {}\n",
    "for supplier in suppliers_list:\n",
    "    # Generate prompt to identify T2 suppliers for each restaurant supplier\n",
    "    output = prompt_chain_function(supplier_template, supplier, 20)\n",
    "    t2_supplier_outputs[supplier] = output\n",
    "\n",
    "# Process and clean T2 supplier outputs\n",
    "for key, value in t2_supplier_outputs.items():\n",
    "    processed_output = process_supplier_info(value)\n",
    "    clean_t2_suppliers = clean_response_function(processed_output)\n",
    "    final_output = process_supplier_info(clean_t2_suppliers)\n",
    "    \n",
    "    # Print T2 supplier outputs\n",
    "    print(f\"{key}'s suppliers: \\n{final_output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6f5d4abc1c852",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now that we can confirm this has worked, and from manual review, the answer is accurate and includes all relevant information, the next stage is to see if we can replicate the idea of where one answer feeds into another, but we will LangChain's LLMChain function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c83fd4927623f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We use the generate_augmented_prompt function defined above \n",
    "# Run that function before continuing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26e9eb9aeafd78cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# First, we are going to use LLMChains\n",
    "# This allows  for the creation of sequences (chains) where the output of one model can serve as the input for another, \n",
    "# enabling complex, multistep information processing workflows.\n",
    "# This is akin to prompt engineering \n",
    "\n",
    "# Step 1: Define the templates\n",
    "template_level1 = \"\"\"\"Identify and list all direct suppliers of {entity}, based on the provided supplier information. \n",
    "It's important to note that we are specifically looking for direct suppliers only, not Tier 2 Suppliers (also known as T2 Suppliers). \n",
    "The output should strictly consist of a comma-separated list of the direct suppliers' names, without any additional text or explanations.\n",
    "\n",
    "For example, the output should look like this: 'Supplier_1, Supplier_2, ..., Supplier_n'.\n",
    "Please ensure the list is formatted exactly as shown in the example, with each supplier's name separated by a comma and a space. \n",
    "\n",
    "Use the following supplier information to support your answer:\n",
    "{supplier_info}\n",
    "\"\"\"\n",
    "\n",
    "template_level2 = \"\"\"\"Please determine and compile a list of all direct suppliers for {entity}, using the detailed supplier information provided. \n",
    "Focus exclusively on direct suppliers, which are specifically called as T2_Suppliers when they supply a supplier. Direct suppliers are entities that directly provide goods or services to {entity}. \n",
    "If no T2_Supplier is found to directly supply {entity}, the response should be left blank.\n",
    "\n",
    "For clarity, if you are asked to identify direct suppliers of Fresh Direct and you encounter information stating, 'A T2_Supplier named Sun Salads directly supplies Watercress in Dorset, UK, to a Supplier named Fresh Direct,' \n",
    "your task is to recognize 'Sun Salads' as the direct supplier to 'Fresh Direct'. Any other information is irrelevant and should not be included in your answer. \n",
    "\n",
    "Your response must be formatted as a comma-separated list containing the names of these direct suppliers, without including any additional text or explanations. \n",
    "\n",
    "Ensure your output adheres to the following format: 'Supplier_1, Supplier_2, ..., Supplier_n'. Each supplier's name should be clearly delineated, separated by a comma followed by a space. \n",
    "Use the following supplier information to support your answer:\n",
    "{supplier_info}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f15c4e60d0d78a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the LLM to use \n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c996d5c44d0113c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Generate the functions \n",
    "def generate_t1_suppliers(template, top_k, user_input_restaurant):\n",
    "    template = template \n",
    "    rag_template = \"\"\"List all of the suppliers of {entity}.\"\"\"\n",
    "    rag_prompt = PromptTemplate(input_variables=[\"entity\"], template=rag_template) \n",
    "\n",
    "    formatted_prompt = rag_prompt.format(entity=user_input_restaurant)\n",
    "    restaurant_suppliers = generate_augmented_prompt(formatted_prompt, top_k=top_k)\n",
    "\n",
    "    # Now we call the llm with the query \n",
    "    prompt = PromptTemplate(input_variables=[\"entity\", \"supplier_info\"], template=template) \n",
    "    suppliers_output = LLMChain(llm=llm, prompt=prompt, output_key='suppliers')\n",
    "    t1_output = suppliers_output.run(entity=user_input_restaurant, supplier_info=restaurant_suppliers)\n",
    "    \n",
    "    return t1_output\n",
    "\n",
    "\n",
    "# The function for tier 2 suppliers\n",
    "def generate_t2_suppliers(template, top_k, output=None):\n",
    "    template = template \n",
    "    rag_template = \"\"\"List all of the T2_Suppliers of {entity}.\"\"\"\n",
    "    rag_prompt = PromptTemplate(input_variables=[\"entity\"], template=rag_template) \n",
    "    # print(\"Going level 2 route\")\n",
    "    suppliers_list = output.split(', ')\n",
    "    t2_supplier_outputs = {}\n",
    "\n",
    "    for supplier in suppliers_list:\n",
    "        # print(supplier)\n",
    "        formatted_prompt = rag_prompt.format(entity=supplier)\n",
    "        supplier_suppliers = generate_augmented_prompt(formatted_prompt, top_k=top_k)\n",
    "\n",
    "        # Now we call the llm with the query \n",
    "        prompt = PromptTemplate(input_variables=[\"entity\", \"supplier_info\"], template=template) \n",
    "        suppliers_output = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "        t2_output = suppliers_output.run(entity=supplier, supplier_info=supplier_suppliers)\n",
    "        t2_supplier_outputs[supplier] = t2_output\n",
    "\n",
    "    return t2_supplier_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93e63890e676affe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the restaurant:  The Chiltern Firehouse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Chiltern Firehouse's Tier 1 Suppliers are: Rare Tea Company\n",
      "\n",
      "\n",
      "{'Rare Tea Company': 'Jun Chiyabari, Satemwa Tea & Coffee Estate'}\n"
     ]
    }
   ],
   "source": [
    "# Run the functions\n",
    "user_input_restaurant = input(\"Enter the name of the restaurant: \")\n",
    "print(\"\\n\")\n",
    "\n",
    "output = generate_t1_suppliers(template=template_level1, top_k=10, user_input_restaurant=user_input_restaurant)\n",
    "\n",
    "print(f\"{user_input_restaurant}'s Tier 1 Suppliers are: {output}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "t2_supplier_outputs = generate_t2_suppliers(template=template_level1, top_k=10, output=output)\n",
    "\n",
    "print(t2_supplier_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd95038d083c31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30d8a8d906e567",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This next section is an example of creating a retrieval-based conversational agent that can handle detailed inquiries by leveraging both document retrieval mechanisms and conversational context. It showcases how chains and memory (in the form of conversational history) are utilized together to create more sophisticated and context-aware responses.\n",
    "\n",
    "So, 'chat_history' is used to store and reference the conversation history as part of the input for the conversational retrival and the code also utilised multiple different types of chains such as 'create_stuff_documents_chain', 'create_retrieval_chain', 'create_history_aware_retriever'.\n",
    "\n",
    "This represents another use case and opportunity to learn how LangChain can be used for this an LLM solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bc0d1bda6946b01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# a) Document chain creation\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# b) Retrieval chain creation\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# c) Conversational Retrieval Chain \n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# d) Example Invocation and Conversational Context Management\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d21eafaeb50655d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 1. Document chain creation \n",
    "# This section initializes a chain for processing documents. It involves creating a template for querying and instantiating a document chain.\n",
    "\n",
    "template = \"\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd9d7bda3b8fd1e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terre Di San Vito, La Sovrana, and Rocca Delle Macìe supply Lina Stores.\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Retrieval Chain Setup\n",
    "# # Setup for a chain dedicated to retrieving documents based on a given query. This includes initializing a retriever and creating a retrieval chain that utilizes the previously defined document chain.\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI Embeddings Model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Set up the vectorstore \n",
    "vectorstore = PineconeVectorStore(\n",
    "    index,\n",
    "    embed_model,\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Testing it works\n",
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"Who supplies Lina Stores?\"\n",
    "})\n",
    "lina_stores_answer = response['answer'] \n",
    "print(lina_stores_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e98b2b809ec14a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 3. Conversational Retrieval Chain Initialization\n",
    "# Creating a chain that is aware of the conversation history for more contextual retrieval. This involves setting up templates and chains that take into account the chat history.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9b5857dabd0c03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response output, including 'chat_history', 'input', 'context' and 'message':\n",
      " {'chat_history': [], 'input': 'Who supplies Lina Stores?', 'context': [Document(page_content='A Supplier named Terre Di San Vito supplies Extra Virgin Olive Oil in the location Puglia to a Restaurant named Lina Stores.'), Document(page_content='A Supplier named La Sovrana supplies Amalfi Lemons, Datterini Tomatoes, Buffalo Burrata, Aubergines in the location New Covent Garden Market, London to a Restaurant named Lina Stores.'), Document(page_content='A Supplier named Rocca Delle Macìe supplies Sangiovese Igt (red Wine), Vermentino Igt (white Wine) in the location Castellina in Chianti, Tuscany, Italy to a Restaurant named Lina Stores.'), Document(page_content='A Supplier named Natoora supplies Produce in the location Not Specified to a Restaurant named Warehouse.'), Document(page_content='A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Bocca Di Lupo.'), Document(page_content='A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Cinnamon Group.'), Document(page_content='A Supplier named Hg Walter supplies Chicken for Breast of Chicken with a Stuffed Wing in the location Not Specified to a Restaurant named Elystan Street.'), Document(page_content='A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Amaya.'), Document(page_content='A Supplier named Lincolnshire Poacher Dairy supplies Dairy Products in the location Ucleby to a Restaurant named Restaurant Sat Bains.'), Document(page_content='A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Zaika.')], 'answer': 'Lina Stores are supplied by Terre Di San Vito, La Sovrana, and Rocca Delle Macìe. Terre Di San Vito supplies Extra Virgin Olive Oil, La Sovrana supplies Amalfi Lemons, Datterini Tomatoes, Buffalo Burrata, Aubergines and Rocca Delle Macìe supplies Sangiovese Igt (red Wine), Vermentino Igt (white Wine).'}\n",
      "\n",
      "Only the response 'message' output:\n",
      " Lina Stores are supplied by Terre Di San Vito, La Sovrana, and Rocca Delle Macìe. Terre Di San Vito supplies Extra Virgin Olive Oil, La Sovrana supplies Amalfi Lemons, Datterini Tomatoes, Buffalo Burrata, Aubergines and Rocca Delle Macìe supplies Sangiovese Igt (red Wine), Vermentino Igt (white Wine).\n",
      "Printing the next answer in the conversation:\n",
      " The text does not provide information on any other restaurants being supplied by Terre Di San Vito, La Sovrana, and Rocca Delle Macìe, apart from Lina Stores.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Example Invocation and Conversational Context Management\n",
    "# This section demonstrates how to invoke the conversational retrieval chain with a specific query and manage the chat history to simulate a dynamic conversation.\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Who supplies Lina Stores?\"),\n",
    "    AIMessage(content=lina_stores_answer)\n",
    "]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me more about it!\"\n",
    "})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "# Manually test \n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': [],\n",
    "    \"input\": \"Who supplies Lina Stores?\"\n",
    "})\n",
    "\n",
    "message_answer = response['answer']\n",
    "\n",
    "print(f\"The response output, including 'chat_history', 'input', 'context' and 'message':\\n {response}\\n\")\n",
    "\n",
    "print(f\"Only the response 'message' output:\\n {message_answer}\")\n",
    "\n",
    "# After first invocation\n",
    "chat_history.append(HumanMessage(content=\"Tell me more about it!\"))\n",
    "chat_history.append(AIMessage(content=message_answer))  # Assuming you add the actual response\n",
    "\n",
    "# Now the chat_history includes the new exchange, and we can invoke the chain again with updated context\n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    \"input\": \"Can you find any other restaurants supplied by those suppliers\"\n",
    "})\n",
    "\n",
    "answer = response['answer']\n",
    "print(f\"Printing the next answer in the conversation:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da307eeb6d1995d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  Who supplies Lina Stores?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Lina Stores is supplied by three different suppliers:\n",
      "\n",
      "1. Terre Di San Vito supplies them with Extra Virgin Olive Oil from Puglia.\n",
      "\n",
      "2. Rocca Delle Macìe supplies Sangiovese Igt (red Wine), Vermentino Igt (white Wine) from Castellina in Chianti, Tuscany, Italy.\n",
      "\n",
      "3. La Sovrana supplies them with Amalfi Lemons, Datterini Tomatoes, Buffalo Burrata, Aubergines from New Covent Garden Market, London.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  Do any of those suppliers supply other restaurants? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: No, according to the provided information, the suppliers Terre Di San Vito, La Sovrana, and Rocca Delle Macìe only supply to Lina Stores. They do not supply to any other restaurants mentioned in the context.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  Who supplies The Chiltern Firehouse directly?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The Chiltern Firehouse is directly supplied by a supplier named Rare Tea Company, which supplies Tea. The location of supply is not specified.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  And who supplies that supplier?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The supplier named Rare Tea Company is supplied by a T2_Supplier named Jun Chiyabari. Jun Chiyabari supplies Tea from the location Dhankuta, Eastern Nepal.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Conversational Loop (Interactive Querying)\n",
    "# Interactive loop allowing users to ask questions in a conversational context. This section demonstrates how the system can handle live user input and respond accordingly.\n",
    "\n",
    "# Given that the conversational_retrieval_chain is already defined and initialized\n",
    "\n",
    "# Define the function to ask a question, reference chat history and retrieval_chain (from Pinecone vector database), print the answer and update chat history\n",
    "\n",
    "def ask_question(input_question):\n",
    "    global chat_history  # Reference the global chat history\n",
    "\n",
    "    # Invoke the conversational retrieval chain with the current chat history and the input question\n",
    "    response = conversational_retrieval_chain.invoke({\n",
    "        'chat_history': chat_history,\n",
    "        \"input\": input_question\n",
    "    })\n",
    "\n",
    "    answer = response['answer']\n",
    "    print(f\"AI: {answer}\")\n",
    "\n",
    "    # Update chat history with the new exchange\n",
    "    chat_history.append(HumanMessage(content=input_question))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "\n",
    "# Interactive loop for asking questions\n",
    "while True:\n",
    "    user_input = input(\"Ask a question: \")  # Get input question from the user\n",
    "    if user_input.lower() == \"quit\":  # Define a way to exit the loop\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    ask_question(user_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc03641f2764d24",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Indexes \n",
    "* Document loaders \n",
    "* Vector databases\n",
    "* Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6b1c5bb5b51cc67",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# a) vector database \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# b) document loaders \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# c) text splitters \n",
    "from langchain.text_splitter  import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0808dc13f59f93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### (a) Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "461a3f5f0acb5b12",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Supplier named Terre Di San Vito supplies Extra Virgin Olive Oil in the location Puglia to a Restaurant named Lina Stores.'),\n",
       " Document(page_content='A Supplier named La Sovrana supplies Amalfi Lemons, Datterini Tomatoes, Buffalo Burrata, Aubergines in the location New Covent Garden Market, London to a Restaurant named Lina Stores.'),\n",
       " Document(page_content='A Supplier named Rocca Delle Macìe supplies Sangiovese Igt (red Wine), Vermentino Igt (white Wine) in the location Castellina in Chianti, Tuscany, Italy to a Restaurant named Lina Stores.'),\n",
       " Document(page_content='A Supplier named Natoora supplies Produce in the location Not Specified to a Restaurant named Warehouse.'),\n",
       " Document(page_content='A Supplier named Afropol supplies Fine Food Products in the location Not Specified to a Restaurant named Bocca Di Lupo.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI Embeddings Model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Set up the vectorstore \n",
    "vectorstore = PineconeVectorStore(\n",
    "    index,\n",
    "    embed_model,\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"Who supplies Lina Stores??\"\n",
    "\n",
    "# Test functionality to retrieve data from Pinecone directly using LangChain\n",
    "vectorstore.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447d1dc1ea109e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### (b) Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8671c896f2c6df5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigating the Complexity of Sustainable Food Supply Chains\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop of pageAboutOur Mission and StrategyOur StoryOur TeamAccountabilityOur WorkWhy Sustainable Agriculture?Our Strategy and ProcessOur Signature ProgramsOur ProjectsNestlé - SAN PartnershipOur ServicesCorporate ESG ServicesLandscape ManagementBiodiversity RestorationCollective Impact OrchestrationNetworkMembershipGovernancePartnershipsClients and donorsToolsSustainable Agriculture FrameworkIntelligence HubBlueprintIPM ResourcesBlogMoreUse tab to navigate through the menu items.DONATE\\n\\n\\n\\n\\nAll PostsMembersLog in / Sign upCommunicationsOct 11, 20232 min readNavigating the Complexity of Sustainable Food Supply ChainsSustainability is no longer just a buzzword; it's a global imperative. As consumers become increasingly conscious of the environmental and social impacts of their choices, the food industry has come under intense scrutiny. One of the biggest challenges it faces today is managing the intricate web of supply chains while striving for sustainability. Supply chain complexity has become a growing challenge for the food industry, and actionable strategies have become key to address this critical issue.\\xa0Consider supporting our global efforts in advancing sustainable and regenerative agriculture.Donate to SAN\\xa0The Growing Complexity of Food Supply ChainsThe food industry has evolved into a complex network of producers, processors, distributors, and retailers, often spanning the globe. This complexity has arisen due to several factors:Globalization: Food supply chains now extend across continents. Products can travel thousands of miles before reaching consumers, making it harder to trace the origins of ingredients and assess their environmental and social impact.Demand for Variety: Consumers today expect a wide range of products year-round. This leads to increased diversity in sourcing, with food companies seeking ingredients from various regions to meet consumer demands.Regulatory Compliance: Stringent regulations related to food safety, quality, and labeling further contribute to the complexity. Complying with these regulations while maintaining sustainability standards can be challenging.Challenges in Sustaining Supply Chain SustainabilityTransparency and Traceability: Ensuring sustainability in the food supply chain requires transparency and traceability. Companies must know where their ingredients come from, how they are produced, and their impact on the environment and local communities. Achieving this in complex global supply chains can be daunting.Resource Constraints: Sustainable practices often require additional resources, such as organic farming methods or fair labor practices. Balancing sustainability with cost-effectiveness is a constant challenge.Climate Change: Climate change affects crop yields, disrupts transportation routes, and increases the vulnerability of supply chains. Adapting to these changes while maintaining sustainability goals is critical.Addressing the ChallengeCollaborate with trusted experts: The Sustainable Agriculture Network provides tailor-made solutions to clients in the food industry, so that challenges in supply chains can be overcome with cost-efficiency and agility.Supply Chain Visibility: Invest in technology and data systems that provide real-time visibility into your supply chain. Blockchain, IoT sensors, and digital platforms can help track products from farm to fork.Supplier Engagement: Collaborate closely with suppliers to implement sustainable practices. Encourage them to adopt environmentally friendly and socially responsible approaches.Local Sourcing: Whenever possible, source ingredients locally. Reducing the distance between production and consumption not only lowers carbon emissions but also supports local communities.Circular Economy: Explore circular economy principles, such as reducing waste and reusing materials. Initiatives like reducing food waste and utilizing byproducts can be both sustainable and cost-effective.Consumer Education: Educate consumers about the environmental and social impact of their food choices. Empower them to make informed decisions that support sustainability.The food industry's quest for sustainability is undoubtedly challenging in today's complex supply chain landscape. However, it is not insurmountable. By prioritizing transparency, supplier engagement, and adherence to sustainability standards, food companies can navigate the intricacies of their supply chains while working towards a more sustainable future. The shift towards sustainability in the food industry is not just a trend; it's a necessity, and it begins with addressing the complexity of our supply chains.500 views0 commentsPost not marked as likedRecent PostsSee AllCircular Economy Principles Applied to Agriculture: From Farm to Fork60Post not marked as likedThe Importance of Pollinators in Sustainable Agriculture950Post not marked as likedAddressing Food Security Through Sustainable Agricultural Practices1490Post not marked as likedinfo@san.eco©2023 by Sustainable Agriculture Network.\\xa0Copyright NoticeAccessibility StatementHomeOur Mission and StrategyOur TeamBlogGet in TouchPrivacy NoticeDonation Refund Policybottom of page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://www.sustainableagriculture.eco/post/navigating-the-complexity-of-sustainable-food-supply-chains', 'title': 'Navigating the Complexity of Sustainable Food Supply Chains', 'language': 'en'})]\n"
     ]
    }
   ],
   "source": [
    "# Additional information to write up a sustainability report \n",
    "# This is for a more comprehensive example use case \n",
    "\n",
    "loader = WebBaseLoader(\"https://www.sustainableagriculture.eco/post/navigating-the-complexity-of-sustainable-food-supply-chains\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6085795f70957c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### (c) Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a2d1293bbac5009",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Navigating the Complexity of Sustainable Food Supply Chains\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop of pageAboutOur Mission and StrategyOur StoryOur TeamAccountabilityOur WorkWhy Sustainable Agriculture?Our Strategy and ProcessOur Signature ProgramsOur ProjectsNestlé - SAN PartnershipOur ServicesCorporate ESG ServicesLandscape ManagementBiodiversity RestorationCollective Impact OrchestrationNetworkMembershipGovernancePartnershipsClients and donorsToolsSustainable Agriculture FrameworkIntelligence HubBlueprintIPM ResourcesBlogMoreUse tab to navigate through the menu items.DONATE', metadata={'source': 'https://www.sustainableagriculture.eco/post/navigating-the-complexity-of-sustainable-food-supply-chains', 'title': 'Navigating the Complexity of Sustainable Food Supply Chains', 'language': 'en'}), Document(page_content=\"All PostsMembersLog in / Sign upCommunicationsOct 11, 20232 min readNavigating the Complexity of Sustainable Food Supply ChainsSustainability is no longer just a buzzword; it's a global imperative. As consumers become increasingly conscious of the environmental and social impacts of their choices, the food industry has come under intense scrutiny. One of the biggest challenges it faces today is managing the intricate web of supply chains while striving for sustainability. Supply chain complexity has become a growing challenge for the food industry, and actionable strategies have become key to address this critical issue.\\xa0Consider supporting our global efforts in advancing sustainable and regenerative agriculture.Donate to SAN\\xa0The Growing Complexity of Food Supply ChainsThe food industry has evolved into a complex network of producers, processors, distributors, and retailers, often spanning the globe. This complexity has arisen due to several factors:Globalization: Food supply chains now extend across continents. Products can travel thousands of miles before reaching consumers, making it harder to trace the origins of ingredients and assess their environmental and social impact.Demand for Variety: Consumers today expect a wide range of products year-round. This leads to increased diversity in sourcing, with food companies seeking ingredients from various regions to meet consumer demands.Regulatory Compliance: Stringent regulations related to food safety, quality, and labeling further contribute to the complexity. Complying with these regulations while maintaining sustainability standards can be challenging.Challenges in Sustaining Supply Chain SustainabilityTransparency and Traceability: Ensuring sustainability in the food supply chain requires transparency and traceability. Companies must know where their ingredients come from, how they are produced, and their impact on the environment and local communities. Achieving this in complex global supply chains can be daunting.Resource Constraints: Sustainable practices often require additional resources, such as organic farming methods or fair labor practices. Balancing sustainability with cost-effectiveness is a constant challenge.Climate Change: Climate change affects crop yields, disrupts transportation routes, and increases the vulnerability of supply chains. Adapting to these changes while maintaining sustainability goals is critical.Addressing the ChallengeCollaborate with trusted experts: The Sustainable Agriculture Network provides tailor-made solutions to clients in the food industry, so that challenges in supply chains can be overcome with cost-efficiency and agility.Supply Chain Visibility: Invest in technology and data systems that provide real-time visibility into your supply chain. Blockchain, IoT sensors, and digital platforms can help track products from farm to fork.Supplier Engagement: Collaborate closely with suppliers to implement sustainable practices. Encourage them to adopt environmentally friendly and socially responsible approaches.Local Sourcing: Whenever possible, source ingredients locally. Reducing the distance between production and consumption not only lowers carbon emissions but also supports local communities.Circular Economy: Explore circular economy principles, such as reducing waste and reusing materials. Initiatives like reducing food waste and utilizing byproducts can be both sustainable and cost-effective.Consumer Education: Educate consumers about the environmental and social impact of their food choices. Empower them to make informed decisions that support sustainability.The food industry's quest for sustainability is undoubtedly challenging in today's complex supply chain landscape. However, it is not insurmountable. By prioritizing transparency, supplier engagement, and adherence to sustainability standards, food companies can navigate the intricacies of their supply chains while working towards a more sustainable future. The shift towards\", metadata={'source': 'https://www.sustainableagriculture.eco/post/navigating-the-complexity-of-sustainable-food-supply-chains', 'title': 'Navigating the Complexity of Sustainable Food Supply Chains', 'language': 'en'}), Document(page_content=\"supplier engagement, and adherence to sustainability standards, food companies can navigate the intricacies of their supply chains while working towards a more sustainable future. The shift towards sustainability in the food industry is not just a trend; it's a necessity, and it begins with addressing the complexity of our supply chains.500 views0 commentsPost not marked as likedRecent PostsSee AllCircular Economy Principles Applied to Agriculture: From Farm to Fork60Post not marked as likedThe Importance of Pollinators in Sustainable Agriculture950Post not marked as likedAddressing Food Security Through Sustainable Agricultural Practices1490Post not marked as likedinfo@san.eco©2023 by Sustainable Agriculture Network.\\xa0Copyright NoticeAccessibility StatementHomeOur Mission and StrategyOur TeamBlogGet in TouchPrivacy NoticeDonation Refund Policybottom of page\", metadata={'source': 'https://www.sustainableagriculture.eco/post/navigating-the-complexity-of-sustainable-food-supply-chains', 'title': 'Navigating the Complexity of Sustainable Food Supply Chains', 'language': 'en'})]\n"
     ]
    }
   ],
   "source": [
    "# Here let's split the docs up into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# print statement for progress\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22535e00bbf0301",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Example use case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91e126c66a1026e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "from tqdm.auto import tqdm  # For progress indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75fe73b2f25fd5d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# First we have to add all the data from documents into the Pinecone database \n",
    "\n",
    "# Step 1: Generate embeddings for your documents\n",
    "# Split documents might have resulted in a list of strings (documents)\n",
    "# Let's generate embeddings for these documents\n",
    "embeddings_list = []\n",
    "for doc in tqdm(documents):\n",
    "    embedding = embed_model.embed_text(doc) \n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "# Step 2: Prepare data for Pinecone\n",
    "# Creating a unique identifier for each document \n",
    "data_to_upsert = []\n",
    "for idx, (doc, emb) in enumerate(zip(documents, embeddings_list)):\n",
    "    unique_id = f\"doc_{idx}\"  \n",
    "    data_to_upsert.append((unique_id, emb, {\"text\": doc}))\n",
    "\n",
    "# Step 3: Upsert data into Pinecone\n",
    "# This can be done in batches to avoid overloading memory with large datasets\n",
    "batch_size = 100 \n",
    "for i in tqdm(range(0, len(data_to_upsert), batch_size)):\n",
    "    batch = data_to_upsert[i:i+batch_size]\n",
    "    index.upsert(vectors=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142e66a5c100b4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Here we want to use the output so the LLM can act as sustainability officer for a restaurant \n",
    "# The final text is a sustainability paragraph on behalf of the restaurant \n",
    "\n",
    "# First let's create a system message to prime the model\n",
    "primer = f\"\"\"\n",
    "You are a sustainability officer for a restaurant, tasked with understanding and optimizing the supply chain for both environmental sustainability and human rights. \n",
    "Your role involves assessing the sources of ingredients used in the restaurant, focusing on the transparency and sustainability of these sources.\n",
    "\n",
    "If comprehensive details of the supply chain are available, utilize this information to highlight sustainable practices or identify areas for improvement. \n",
    "In cases where information about suppliers or their practices is incomplete, note the need to seek out this information to enhance supply chain transparency.\n",
    "\n",
    "Your approach should include:\n",
    "- Identifying known suppliers and their contributions to the restaurant's supply chain.\n",
    "- Acknowledging gaps in the supply chain information, especially regarding secondary suppliers or the origins of specific ingredients.\n",
    "- Emphasizing the importance of continuous improvement and investigation to ensure the sustainability and ethical standards of the supply chain are met.\n",
    "- Write 1-2 Paragraphs that can be included in the restaurant's annual sustainability report.\n",
    "\"\"\"\n",
    "\n",
    "# The function generate_response_with_rag needs to be ran before continuing \n",
    "\n",
    "# Combining the information\n",
    "# This info is taken from the prompt engineering approach \n",
    "combined_info = restaurant_suppliers + \"\\n\\nTier 2 Supplier Information:\\n\"\n",
    "\n",
    "for supplier, details in t2_supplier_outputs.items():\n",
    "    combined_info += f\"\\n{supplier}:\\n{details}\"\n",
    "\n",
    "generate_response_with_rag(combined_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191ef102df7ceb7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d21546e47b2a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ai8-sym": {
   "notebook_id": "88e4de2c-f269-421e-bdc4-8e41ee6adc0f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
